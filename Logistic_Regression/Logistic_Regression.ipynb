{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-02T11:37:46.909938Z",
     "start_time": "2025-03-02T11:37:46.906420Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from Logistic_Regression import LogisticRegression"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T11:37:46.925737Z",
     "start_time": "2025-03-02T11:37:46.917166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target"
   ],
   "id": "f9b3420fdc95109a",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T11:37:46.942593Z",
     "start_time": "2025-03-02T11:37:46.936558Z"
    }
   },
   "cell_type": "code",
   "source": "X[:5]",
   "id": "94e45053d143928c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,\n",
       "        3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,\n",
       "        8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,\n",
       "        3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,\n",
       "        1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, 1.326e+03, 8.474e-02, 7.864e-02,\n",
       "        8.690e-02, 7.017e-02, 1.812e-01, 5.667e-02, 5.435e-01, 7.339e-01,\n",
       "        3.398e+00, 7.408e+01, 5.225e-03, 1.308e-02, 1.860e-02, 1.340e-02,\n",
       "        1.389e-02, 3.532e-03, 2.499e+01, 2.341e+01, 1.588e+02, 1.956e+03,\n",
       "        1.238e-01, 1.866e-01, 2.416e-01, 1.860e-01, 2.750e-01, 8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, 1.203e+03, 1.096e-01, 1.599e-01,\n",
       "        1.974e-01, 1.279e-01, 2.069e-01, 5.999e-02, 7.456e-01, 7.869e-01,\n",
       "        4.585e+00, 9.403e+01, 6.150e-03, 4.006e-02, 3.832e-02, 2.058e-02,\n",
       "        2.250e-02, 4.571e-03, 2.357e+01, 2.553e+01, 1.525e+02, 1.709e+03,\n",
       "        1.444e-01, 4.245e-01, 4.504e-01, 2.430e-01, 3.613e-01, 8.758e-02],\n",
       "       [1.142e+01, 2.038e+01, 7.758e+01, 3.861e+02, 1.425e-01, 2.839e-01,\n",
       "        2.414e-01, 1.052e-01, 2.597e-01, 9.744e-02, 4.956e-01, 1.156e+00,\n",
       "        3.445e+00, 2.723e+01, 9.110e-03, 7.458e-02, 5.661e-02, 1.867e-02,\n",
       "        5.963e-02, 9.208e-03, 1.491e+01, 2.650e+01, 9.887e+01, 5.677e+02,\n",
       "        2.098e-01, 8.663e-01, 6.869e-01, 2.575e-01, 6.638e-01, 1.730e-01],\n",
       "       [2.029e+01, 1.434e+01, 1.351e+02, 1.297e+03, 1.003e-01, 1.328e-01,\n",
       "        1.980e-01, 1.043e-01, 1.809e-01, 5.883e-02, 7.572e-01, 7.813e-01,\n",
       "        5.438e+00, 9.444e+01, 1.149e-02, 2.461e-02, 5.688e-02, 1.885e-02,\n",
       "        1.756e-02, 5.115e-03, 2.254e+01, 1.667e+01, 1.522e+02, 1.575e+03,\n",
       "        1.374e-01, 2.050e-01, 4.000e-01, 1.625e-01, 2.364e-01, 7.678e-02]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T11:37:46.989610Z",
     "start_time": "2025-03-02T11:37:46.985617Z"
    }
   },
   "cell_type": "code",
   "source": "y[:5]",
   "id": "f07492c12b3afe2b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T11:37:47.041213Z",
     "start_time": "2025-03-02T11:37:47.034212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ],
   "id": "b8c1f76bf46ece38",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (455,), (114, 30), (114,))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "Logistic regression is a widely used model in machine learning for binary classification tasks. It models the probability that a given input belongs to a particular class. The logistic regression model function is represented as:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = g(\\mathbf{w} \\cdot \\mathbf{x} + b) $$\n",
    "\n",
    "In this equation, $f_{\\mathbf{w},b}(\\mathbf{x})$ represents the predicted probability, $\\mathbf{w}$ is the weight vector, $\\mathbf{b}$ is the bias term, $\\mathbf{x}$ is the input feature vector, and $g(z)$ is the sigmoid function:\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "## Model Training\n",
    "\n",
    "To train a logistic regression model, we aim to find the best values for the parameters $(\\mathbf{w}, b)$ that best fit our dataset and provide accurate class probabilities.\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "The forward pass computes the linear combination of input features $\\mathbf{x}$ with the weight vector $\\mathbf{w}$ and the bias term $b$ and then applies the sigmoid function to the result:\n",
    "\n",
    "$$ Z = \\mathbf{x} \\cdot \\mathbf{w} + b $$\n",
    "$$ A = \\sigma(Z) $$\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "The cost function measures the error between the predicted probabilities and the true labels. In logistic regression, we use the binary cross-entropy loss function:\n",
    "\n",
    "$$ J(\\mathbf{w},b) = -\\frac{1}{m} \\sum_{i=0}^{m-1} \\left[y_i \\log\\left(f_{\\mathbf{w},b}(\\mathbf{x}_i)\\right) + (1 - y_i) \\log\\left(1 - f_{\\mathbf{w},b}(\\mathbf{x}_i)\\right)\\right] $$\n",
    "\n",
    "Here, $m$ is the number of samples, $y_i$ is the true label of sample $i$, and $f_{\\mathbf{w},b}(\\mathbf{x}_i)$ is the predicted probability of sample $i$ belonging to the positive class.\n",
    "\n",
    "### Backward Pass (Gradient Computation)\n",
    "\n",
    "The backward pass calculates the gradients of the cost function with respect to the parameters $(\\mathbf{w}, b)$. These gradients are essential for updating the model parameters during training:\n",
    "\n",
    "$$ \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left(f_{\\mathbf{w},b}(\\mathbf{x}_i) - y_i\\right) $$\n",
    "\n",
    "$$ \\frac{\\partial J(\\mathbf{w},b)}{\\partial \\mathbf{w}} = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left(f_{\\mathbf{w},b}(\\mathbf{x}_i) - y_i\\right)\\mathbf{x}_i $$\n",
    "\n",
    "## Training Process\n",
    "\n",
    "The training process involves iteratively updating the weight vector $\\mathbf{w}$ and bias term $b$ to minimize the cost function. This is typically done through an optimization algorithm like gradient descent. The update equations for parameters are:\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{w}} $$\n",
    "\n",
    "$$ b \\leftarrow b - \\alpha \\frac{\\partial J}{\\partial b} $$\n",
    "\n",
    "Here, $\\alpha$ represents the learning rate, which controls the step size during parameter updates.\n",
    "\n",
    "By iteratively performing the forward pass, computing the cost, performing the backward pass, and updating the parameters, the logistic regression model learns to make better predictions and fit the data.\n"
   ],
   "id": "e8e2da635985142d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clf = LogisticRegression(learning_rate = 0.01, n_iters = 1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ],
   "id": "8d8606eec7c38ef6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "**In classification tasks, it's crucial to evaluate the performance of your model. There are several metrics that can help you understand how well your model is performing. Here are four commonly used classification metrics:**\n",
    "## Accuracy\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Accuracy** measures the proportion of correctly predicted instances out of all instances in a classification model.\n",
    "- It is a widely used metric for evaluating classification performance.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher accuracy value indicates a better classification model.\n",
    "- However, accuracy alone may not provide a complete picture, especially in imbalanced datasets.\n",
    "\n",
    "## Precision\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Precision** measures the proportion of true positive predictions out of all positive predictions made by the model.\n",
    "- It is a useful metric when the cost of false positives is high.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher precision means the model makes fewer false positive predictions.\n",
    "\n",
    "## Recall (Sensitivity)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Recall (Sensitivity)} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Recall**, also known as sensitivity or true positive rate, measures the proportion of true positive predictions out of all actual positive instances in the dataset.\n",
    "- It is a valuable metric when it's essential to capture all positive instances.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher recall means the model captures more of the actual positive instances.\n",
    "\n",
    "## F1-Score\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- The **F1-Score** is the harmonic mean of precision and recall.\n",
    "- It provides a balance between precision and recall, making it a suitable metric when there is a trade-off between false positives and false negatives.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher F1-Score indicates a model that achieves a balance between precision and recall.\n",
    "\n"
   ],
   "id": "69a4c5927dbed96e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T11:37:47.185259500Z",
     "start_time": "2025-03-02T11:36:04.487323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ClassificationMetrics:\n",
    "    @staticmethod\n",
    "    def accuracy(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes the accuracy of a classification model.\n",
    "\n",
    "        Parameters:\n",
    "        y_true (numpy array): A numpy array of true labels for each data point.\n",
    "        y_pred (numpy array): A numpy array of predicted labels for each data point.\n",
    "\n",
    "        Returns:\n",
    "        float: The accuracy of the model, expressed as a percentage.\n",
    "        \"\"\"\n",
    "\n",
    "        y_true = y_true.flatten()\n",
    "        total_samples = len(y_true)\n",
    "        correct_predictions = np.sum(y_true == y_pred)\n",
    "        return correct_predictions / total_samples\n",
    "\n",
    "    @staticmethod\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes the precision of a classification model.\n",
    "\n",
    "        Parameters:\n",
    "        y_true (numpy array): A numpy array of true labels for each data point.\n",
    "        y_pred (numpy array): A numpy array of predicted labels for each data point.\n",
    "\n",
    "        Returns:\n",
    "        float: The precision of the model, which measures the proportion of true positive predictions\n",
    "        out of all positive predictions made by the model.\n",
    "        \"\"\"\n",
    "        true_positives = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        false_positives = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        return true_positives / (true_positives + false_positives)\n",
    "\n",
    "    @staticmethod\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes the recall (sensitivity) of a classification model.\n",
    "\n",
    "        Parameters:\n",
    "        y_true (numpy array): A numpy array of true labels for each data point.\n",
    "        y_pred (numpy array): A numpy array of predicted labels for each data point.\n",
    "\n",
    "        Returns:\n",
    "        float: The recall of the model, which measures the proportion of true positive predictions\n",
    "        out of all actual positive instances in the dataset.\n",
    "        \"\"\"\n",
    "        true_positives = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes the F1-score of a classification model.\n",
    "\n",
    "        Parameters:\n",
    "        y_true (numpy array): A numpy array of true labels for each data point.\n",
    "        y_pred (numpy array): A numpy array of predicted labels for each data point.\n",
    "\n",
    "        Returns:\n",
    "        float: The F1-score of the model, which is the harmonic mean of precision and recall.\n",
    "        \"\"\"\n",
    "        precision = ClassificationMetrics.precision(y_true, y_pred)\n",
    "        recall = ClassificationMetrics.recall(y_true, y_pred)\n",
    "        return 2 * (precision * recall) / (precision + recall)"
   ],
   "id": "7b3e4a5943b49692",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "accuracy = ClassificationMetrics.accuracy(y_test, y_pred)\n",
    "precision = ClassificationMetrics.precision(y_test, y_pred)\n",
    "recall = ClassificationMetrics.recall(y_test, y_pred)\n",
    "f1_score = ClassificationMetrics.f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Precision: {precision:.2%}\")\n",
    "print(f\"Recall: {recall:.2%}\")\n",
    "print(f\"F1-score: {f1_score:.2%}\")"
   ],
   "id": "5fba4931c14561b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
