{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Overview and Import\n",
    "**Recurrent Neural Networks (RNNs) are a type of neural network that are designed to handle sequential data, such as time series or natural language text. They achieve this by using a hidden state that is updated for each time step of the input sequence, allowing the network to maintain a memory of previous inputs.**\n",
    "\n",
    "\n",
    "**This notebook contains an implementation of an RNN that can be used for language modeling. The self takes in a sequence of characters and outputs the probability distribution over the next character in the sequence. The network is trained on a corpus of text and then used to generate new text that has a similar distribution of characters as the training corpus.**"
   ],
   "id": "4f13fbe8e6aea261"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-13T06:39:04.880932Z",
     "start_time": "2025-05-13T06:39:04.808051Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "\n",
    "from RNN import RNN"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Prepartion",
   "id": "c44de0361dd98aae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T06:39:05.041571Z",
     "start_time": "2025-05-13T06:39:05.034739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    A class for generating input and output examples for a character-level language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        Initialize a DataGenerator object.\n",
    "\n",
    "        Arg:\n",
    "            path (str): The path to the text file containing the training data.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "\n",
    "        # Read in data from file and convert to lowercase\n",
    "        with open(path) as f:\n",
    "            data = f.read().lower()\n",
    "\n",
    "        # create list of unique characters in the data\n",
    "        self.chars = list(set(data))\n",
    "\n",
    "        # Create dictionaries mapping characters to and from their index in the list of unique characters\n",
    "        self.char_to_idx = {ch: i for (i, ch) in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for (i, ch) in enumerate(self.chars)}\n",
    "\n",
    "        # Set the size of the vocabulary (i.e. number of unique characters)\n",
    "        self.vocab_size = len(self.chars)\n",
    "\n",
    "        # Read in examples from file and covert to lowercase, removing leading/training while space\n",
    "        with open(path) as f:\n",
    "            examples = f.readlines()\n",
    "        self.examples = [x.lower().strip() for x in examples]\n",
    "\n",
    "    def generate_example(self, idx):\n",
    "        \"\"\"\n",
    "        Generate an input/output example for the language model based on the given index.\n",
    "\n",
    "        Arg:\n",
    "            idx (int): The index of the example to generate.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the input and output arrays for the example.\n",
    "        \"\"\"\n",
    "        example_chars = self.examples[idx]\n",
    "\n",
    "        # Convert the characters in the example to their corresspoding indices in the list of unique characters\n",
    "        example_char_idx = [self.char_to_idx[char] for char in example_chars]\n",
    "\n",
    "        # Add newline character as the first characters in the input array, and as the last characters in the output array\n",
    "        X = [self.char_to_idx['\\n']] + example_char_idx\n",
    "        Y = example_char_idx + [self.char_to_idx['\\n']]\n",
    "\n",
    "        return np.array(X), np.array(Y)"
   ],
   "id": "4563279d24a6027b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RNN Implementation",
   "id": "d229c4c73b823ebd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**The RNN used in this notebook is a basic one-layer RNN. It consists of an input layer, a hidden layer, and an output layer. The input layer takes in a one-hot encoded vector representing a character in the input sequence. This vector is multiplied by a weight matrix  $W_{ax}$ to produce a hidden state vector $a$. The hidden state vector is then passed through a non-linear activation function (in this case, the hyperbolic tangent function) and updated for each time step of the input sequence. The updated hidden state is then multiplied by a weight matrix  $W_{ya}$ to produce the output probability distribution over the next character in the sequence.**\n",
    "\n",
    "**The RNN is trained using stochastic gradient descent with the cross-entropy loss function. During training, the self takes in a sequence of characters and outputs the probability distribution over the next character. The true next character is then compared to the predicted probability distribution, and the parameters of the network are updated to minimize the cross-entropy loss.**"
   ],
   "id": "3882d2b174fea887"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Activation Functions\n",
    "### Softmax Activation Function\n",
    "\n",
    "**$$\\mathrm{softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}\n",
    "$$**\n",
    "\n",
    "**The softmax function is commonly used as an activation function in neural networks, particularly in the output layer for classification tasks. Given an input array $x$, the softmax function calculates the probability distribution of each element in the array**\n",
    "\n",
    "### Tanh Activation\n",
    "**$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$**\n",
    "\n",
    "**where $x$ is the input to the function. The output of the function is a value between -1 and 1. The tanh activation function is often used in neural networks as an alternative to the sigmoid activation function, as it has a steeper gradient and can better model non-linear relationships in the data.**\n",
    "****"
   ],
   "id": "1ffe389d37aa75e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Forward propagation:\n",
    "\n",
    "**During forward propagation, the input sequence is processed through the RNN to generate an output sequence. At each time step, the hidden state and the output are computed using the input, the previous hidden state, and the RNN's parameters.**\n",
    "\n",
    "**The equations for the forward propagation in a basic RNN are as follows:**\n",
    "\n",
    "**At time step $t$, the input to the RNN is $x_t$, and the hidden state at time step $t-1$ is $a_{t-1}$. The hidden state at time step $t$ is computed as:**\n",
    "\n",
    "**$a_t = \\tanh(W_{aa} a_{t-1} + W_{ax} x_t + b_a)$**\n",
    "\n",
    "**where $W_{aa}$ is the weight matrix for the hidden state, $W_{ax}$ is the weight matrix for the input, and $b_a$ is the bias vector for the hidden state.**\n",
    "\n",
    "**The output at time step $t$ is computed as:**\n",
    "\n",
    "**$y_t = softmax(W_{ya} a_t + b_y)$**\n",
    "\n",
    "**where $W_{ya}$ is the weight matrix for the output, and $b_y$ is the bias vector for the output.**\n",
    "****"
   ],
   "id": "d5e3ad08205f68ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Backward propagation:\n",
    "\n",
    "**The objective of training an RNN is to minimize the loss between the predicted sequence and the ground truth sequence. Backward propagation calculates the gradients of the loss with respect to the RNN's parameters, which are then used to update the parameters using an optimization algorithm such as Adagrad or Adam.**\n",
    "\n",
    "**The equations for the backward propagation in a basic RNN are as follows:**\n",
    "\n",
    "**At time step $t$, the loss with respect to the output $y_t$ is given by:**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial y_t} = -\\frac{1}{y_{t,i}} \\text{ if } i=t_i, \\text{ else } 0$**\n",
    "\n",
    "**where $L$ is the loss function, $y_{t,i}$ is the $i$th element of the output at time step $t$, and $t_i$ is the index of the true label at time step $t$**.\n",
    "\n",
    "**The loss with respect to the hidden state at time step $t$ is given by:**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial a_t} = \\frac{\\partial L}{\\partial y_t} W_{ya} + \\frac{\\partial L}{\\partial h_{t+1}} W_{aa}$**\n",
    "\n",
    "**where $\\frac{\\partial L}{\\partial a_{t+1}}$ is the gradient of the loss with respect to the hidden state at the next time step, which is backpropagated through time.**\n",
    "\n",
    "**The gradient with respect to tanh is given by:**\n",
    "**$\\frac{\\partial \\tanh(a)} {\\partial a}$**\n",
    "\n",
    "**The gradients with respect to the parameters are then computed using the chain rule:**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial W_{ya}} = \\sum_t \\frac{\\partial L}{\\partial y_t} a_t$**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial b_y} = \\sum_t \\frac{\\partial L}{\\partial y_t}$**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial W_{ax}} = \\sum_t \\frac{\\partial L}{\\partial a_t} \\frac{\\partial a_t}{\\partial W_{ax}}$**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial W_{aa}} = \\sum_t \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_{aa}}$**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial b_a} = \\sum_t \\frac{\\partial L}{\\partial a_t} \\frac{\\partial h_t}{\\partial b_a}$**\n",
    "\n",
    "**where $\\frac{\\partial h_t}{\\partial W_{ax}}$, $\\frac{\\partial a_t}{\\partial W_{aa}}$, and $\\frac{\\partial h_t}{\\partial b_a}$ can be computed as:**\n",
    "\n",
    "**$\\frac{\\partial a_t}{\\partial W_{ax}} = x_t$**\n",
    "\n",
    "**$\\frac{\\partial a_t}{\\partial W_{aa}} = a_{t-1}$**\n",
    "\n",
    "**$\\frac{\\partial a_t}{\\partial b_a} = 1$**\n",
    "\n",
    "**These gradients are then used to update the parameters of the RNN using an optimization algorithm such as gradient descent, Adagrad, or Adam.**\n",
    "****"
   ],
   "id": "4c809363106424d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loss:\n",
    "\n",
    "**The cross-entropy loss between the predicted probabilities y_pred and the true targets y_true at a single time step $t$ is:**\n",
    "\n",
    "**$$H(y_{true,t}, y_{pred,t}) = -\\sum_i y_{true,t,i} \\log(y_{pred,t,i})$$**\n",
    "\n",
    "**where $y_{pred,t}$ is the predicted probability distribution at time step $t$, $y_{true,t}$ is the true probability distribution at time step $t$ (i.e., a one-hot encoded vector representing the true target), and $i$ ranges over the vocabulary size.**\n",
    "\n",
    "**The total loss is then computed as the sum of the cross-entropy losses over all time steps:**\n",
    "\n",
    "**$$L = \\sum_{t=1}^{T} H(y_{true,t}, y_{pred,t})$$**\n",
    "\n",
    "**where $T$ is the sequence length.**\n",
    "\n",
    "****\n"
   ],
   "id": "e0463419d2a60326"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train:\n",
    "**The train method trains the RNN on a dataset using backpropagation through time. The method takes an instance of DataReader containing the training data as input. The method initializes a hidden state vector a_prev at the beginning of each sequence to zero. It then iterates until the smooth loss is less than a threshold value.**\n",
    "\n",
    "**During each iteration, it retrieves a batch of inputs and targets from the data reader. The RNN then performs a forward pass on the input sequence and computes the output probabilities. The backward pass is performed using the targets and output probabilities to calculate the gradients of the parameters of the network. The Adagrad algorithm is used to update the weights of the network.**\n",
    "\n",
    "**The method then calculates and updates the loss using the updated weights. The previous hidden state is updated for the next batch. The method prints the progress every 500 iterations by generating a sample of text using the sample method and printing the loss.**\n",
    "\n",
    "\n",
    "**The train method can be summarized by the following steps:**\n",
    "\n",
    "\n",
    "**$1.$ Initialize $a_{prev}$ to zero at the beginning of each sequence.**\n",
    "\n",
    "**$2.$ Retrieve a batch of inputs and targets from the data reader.**\n",
    "\n",
    "**$3.$ Perform a forward pass on the input sequence and compute the output probabilities.**\n",
    "\n",
    "**$4.$ Perform a backward pass using the targets and output probabilities to calculate the gradients of the parameters of the network.**\n",
    "\n",
    "**$5.$ Use the Adagrad algorithm to update the weights of the network.**\n",
    "\n",
    "**$6.$ Calculate and update the loss using the updated weights.**\n",
    "\n",
    "**$7.$ Update the previous hidden state for the next batch.**\n",
    "\n",
    "**$8.$ Print progress every 10000 iterations by generating a sample of text using the sample method and printing the loss.**\n",
    "\n",
    "**$9.$ Repeat steps $2$-$8$ until the smooth loss is less than the threshold value.**"
   ],
   "id": "36acd73a83070358"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T06:39:05.060741Z",
     "start_time": "2025-05-13T06:39:05.054983Z"
    }
   },
   "cell_type": "code",
   "source": "data_generator = DataGenerator('D:\\Machine_Learning_From_Scratch\\RNN\\data\\dinos.txt')",
   "id": "9eb82fd612537084",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T06:39:13.341130Z",
     "start_time": "2025-05-13T06:39:05.075496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rnn = RNN(hidden_size = 200, data_generator = data_generator, sequence_length = 25, learning_rate = 1e-3)\n",
    "rnn.train()"
   ],
   "id": "1a4155cd8e857491",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iter 0, loss 82.359704\n",
      "\n",
      "Ffssyyjjrreehhddssrrbbxxssddvvwwxxddccttnnaaoobbffbbrrxx\n",
      "\n",
      "Ccvvtt\n",
      "\n",
      "Rrqqiilleejjccttjjjjhhsswwvvqqqqsskkkkaaaabboobbuunneejj\n",
      "\n",
      "Ttffwwffnnxxggppddxxzzccvvccrrqqggnnhhzzssuu\n",
      "\n",
      "Iiccbbnnddoorrggiizznnttlluuqqbbqqhhxxssrrxxddttppggtthhyyookkppttjjjjrrttgguuwwnnoossuuhhmmppnniiaa\n",
      "\n",
      "iter 500, loss 58.355189\n",
      "\n",
      "Aannrraatthhooppiiss\n",
      "\n",
      "\n",
      "\n",
      "Ttiirreessaauuss\n",
      "\n",
      "Eennyyllaappaauurruuss\n",
      "\n",
      "Hheenneennyymmaaaa\n",
      "\n",
      "\n",
      "\n",
      "iter 1000, loss 39.471550\n",
      "\n",
      "Aaaarroommvvuuss\n",
      "\n",
      "Lliissaauurruuss\n",
      "\n",
      "Aaeeeelloossaauurruuss\n",
      "\n",
      "Ddaappnnooppaaffrroommiimmuuss\n",
      "\n",
      "Aaaarriimmppeessaauurruuss\n",
      "\n",
      "\n",
      "\n",
      "iter 1500, loss 26.290693\n",
      "\n",
      "Aabbrrooppyyoossaauuss\n",
      "\n",
      "Aaaarrddoouussaauurruuss\n",
      "\n",
      "Aaccaaiitthhoobbaattoorr\n",
      "\n",
      "Nn\n",
      "\n",
      "Aapprroossttaarruuss\n",
      "\n",
      "\n",
      "\n",
      "iter 2000, loss 17.787749\n",
      "\n",
      "Aaccaanntthhoopphhoolliiss\n",
      "\n",
      "Aallrroossaauurruuss\n",
      "\n",
      "Ddaassaauurruuss\n",
      "\n",
      "\n",
      "\n",
      "Aaaaddaallllaahhssaauurruuss\n",
      "\n",
      "\n",
      "\n",
      "iter 2500, loss 12.551106\n",
      "\n",
      "Aammttiioossaauurruuss\n",
      "\n",
      "Aaaattiilllluuss\n",
      "\n",
      "Aammttiissaauurruuss\n",
      "\n",
      "Hheeeeoonnhhrruuss\n",
      "\n",
      "Aabbrroossaauurruuss\n",
      "\n",
      "\n",
      "\n",
      "iter 3000, loss 9.326275\n",
      "\n",
      "Aammeennyyxx\n",
      "\n",
      "Aappttoossaauurruuss\n",
      "\n",
      "Iimmuussaauurruuss\n",
      "\n",
      "Aammhheelloobbaattoorr\n",
      "\n",
      "Aacchhiiccttoorr\n",
      "\n",
      "\n",
      "\n",
      "iter 3500, loss 7.303700\n",
      "\n",
      "Aammaannttiissaauurruuss\n",
      "\n",
      "Aammaattoorr\n",
      "\n",
      "Aahheennyyxxaaffrroommiimmuuss\n",
      "\n",
      "Aammaannttiissaauurruuss\n",
      "\n",
      "Aahheennyyxxaaffrroommaappttoorr\n",
      "\n",
      "\n",
      "\n",
      "iter 4000, loss 6.098037\n",
      "\n",
      "Aammtthhoossaauurruuss\n",
      "\n",
      "Aahheennoossaauurruuss\n",
      "\n",
      "Aammaassaauurruuss\n",
      "\n",
      "Aattoonn\n",
      "\n",
      "Aaiioorrnntthhoossaauurruuss\n",
      "\n",
      "\n",
      "\n",
      "iter 4500, loss 5.409407\n",
      "\n",
      "Aaccttoonnhhiissaauurruuss\n",
      "\n",
      "Aammaannttiissaauurruuss\n",
      "\n",
      "Aahheennaanntteenn\n",
      "\n",
      "Ddaattoonntthhoossaauurruuss\n",
      "\n",
      "Bbaaccttoossaauurruuss\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T06:39:58.171272Z",
     "start_time": "2025-05-13T06:39:58.160102Z"
    }
   },
   "cell_type": "code",
   "source": "rnn.predict('meo')",
   "id": "11f707cf27720df9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meopisaurus'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T06:40:08.550163Z",
     "start_time": "2025-05-13T06:40:08.540667Z"
    }
   },
   "cell_type": "code",
   "source": "rnn.predict('a')",
   "id": "18509fbc2ebaca0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'afroctobator'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d1b7e251fecd7dbb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
