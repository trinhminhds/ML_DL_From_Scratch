{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How the Algorithm Works\n",
    "\n",
    "**The objective of SVM is to find a hyperplane that separates the data into two categories for binary classification.**\n",
    "\n",
    "## Key Points:\n",
    "\n",
    "### Hyperplane\n",
    "A hyperplane is a subspace whose dimension is one less than that of its ambient space. For instance, in an n-dimensional space, the hyperplane is (n-1)-dimensional.\n",
    "\n",
    "For SVMs, the goal is to maximize the margin between the two classes. The equation of the hyperplane can be represented as:\n",
    "\n",
    "$$wx - b = 0$$\n",
    "\n",
    "where $w$ is the weight vector, $x$ is the feature vector, and $b$ is the bias.\n",
    "\n",
    "To classify data points, we use the following conditions:\n",
    "\n",
    "- For a data point with label $y = 1$, we want $wx - b \\geq 1$.\n",
    "- For a data point with label $y = -1$, we want $wx - b \\leq -1$.\n",
    "\n",
    "In general, we aim for:\n",
    "\n",
    "$$y(wx - b) \\geq 1$$\n",
    "\n",
    "for all data points.\n",
    "\n",
    "### Margin\n",
    "\n",
    "The margin is defined as the distance between the hyperplane and the nearest data points from both classes, which are called support vectors. The larger the margin, the better the generalization ability of the classifier.\n",
    "\n",
    "## Gradient Equations\n",
    "\n",
    "The gradients of the cost function with respect to the weights $w$ and bias $b$ are used to update these parameters during the optimization process. The cost function includes a term for maximizing the margin (which depends on $w$) and a regularization term to prevent overfitting.\n",
    "\n",
    "For a data point $x_i$ with label $y_i$ that satisfies $y_i(wx_i - b) \\geq 1$ (correctly classified and outside the margin), the gradients are:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w} = 2\\lambda w$$\n",
    "$$\\frac{\\partial J}{\\partial b} = 0$$\n",
    "\n",
    "For a data point $x_i$ with label $y_i$ that does not satisfy $y_i(wx_i - b) \\geq 1$ (either incorrectly classified or within the margin), the gradients are:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w} = 2\\lambda w - y_ix_i$$\n",
    "$$\\frac{\\partial J}{\\partial b} = -y_i$$\n",
    "\n",
    "Here, $J$ is the cost function of SVM, $\\lambda$ is the regularization parameter (which controls the trade-off between increasing the margin size and ensuring that the $x_i$ lie on the correct side of the margin), $w$ is the weight vector, $b$ is the bias term, $x_i$ is the ith data point, and $y_i$ is the corresponding label. This setup ensures that the classifier not only finds a separating hyperplane (if it exists) but also seeks the one that maximizes the margin between classes, which is central to SVM's classification strategy.\n"
   ],
   "id": "1ff2868199019aef"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-06T05:13:27.170291Z",
     "start_time": "2025-03-06T05:13:13.231356Z"
    }
   },
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_test_split(X, y, test_size, random_state):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "        X (np.ndarray): Features array of shape (n_samples, n_features).\n",
    "        y (np.ndarray): Target array of shape (n_samples, ).\n",
    "        test_size (float): Proportion of samples to include in the test set.\n",
    "        random_state (int): Seed for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray]: A tuple of arrays (X_train, X_test, y_train, y_test).\n",
    "    \"\"\"\n",
    "\n",
    "    # Get number of samples\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Set the seed for the random number generator\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Shuffle the indices\n",
    "    shuffle_indices = np.random.permutation(np.arange(n_samples))\n",
    "\n",
    "    # Determine the size of the test set\n",
    "    test_size = int(n_samples * test_size)\n",
    "\n",
    "    # Split the indices into test and train\n",
    "    test_indices = shuffle_indices[:test_size]\n",
    "    train_indices = shuffle_indices[test_size:]\n",
    "\n",
    "    # Split the features and target into arrays into test and train\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ],
   "id": "60f5e63f92a0079f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fd7744370ce2d98b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
